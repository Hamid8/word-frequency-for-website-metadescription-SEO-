# -*- coding: utf-8 -*-
"""Crawler for keyword(RTC)2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L5TOx-0td-NMNLfjks8q7JOWABu_e80f
"""

!pip install advertools

!pip install --upgrade advertools

import advertools as adv

"""**1.clawling all  URLs under CSV.com/shop website path**

**2.Download the the RAW data of Crawl**
"""

adv.crawl('https://get2theroot.com/', 'my_output_file.jl', follow_links=True)

import pandas as pd
crawl_df = pd.read_json('my_output_file.jl', lines=True)

crawl_df = crawl_df.drop_duplicates('url', keep='last')

crawl_df = crawl_df.reset_index(drop=True)

from google.colab import files
crawl_df.to_csv('cvshealth.csv')
files.download('cvshealth.csv')

crawl_df

crawl_df['url']

"""### Checking the table of data with headers and page SEO  and download the file"""

meta_dec = crawl_df.dropna(subset = ["meta_desc"], inplace=True)

meta_dec.to_csv('metadescription.csv')
files.download('metadescription.csv')

"""###aplying NLP for metadiscription on links"""

crawl_df = crawl_df[crawl_df['meta_desc'].notna()]

crawl_df['meta_desc']

import spacy
spacy.cli.download("en_core_web_md")
nlp = spacy.load("en_core_web_md")

"""##1.remove pounctioation and stop words

##2.spacy model tokenize and creating entity column and then detokenize and

---

##3.aggregate new in the same date
"""

crawl_df['meta_desc'] = crawl_df.meta_desc.apply(lambda text:  " ".join(token.lemma_ for token in nlp(text) if not token.is_stop))
crawl_df['meta_desc']  = crawl_df.meta_desc.apply(lambda text:  " ".join(token.lemma_ for token in nlp(text) if not token.is_punct))

crawl_df['meta_desc'] = crawl_df['meta_desc'].apply(lambda x: nlp(x))



crawl_df['meta_desc'] = crawl_df['meta_desc'].apply(lambda x: x.text)

"""1. It takes a string as input
2. It tokenizes the string using spaCy
3. It lemmatizes each token
4. It returns a string where each token is separated by a space
"""

def make_to_base(x):
    x_list = []
    for token in nlp(x):
        lemma=str(token.lemma_)
        if lemma=='-PRON-' or lemma=='be':
            lemma=token.text
        x_list.append(lemma)
    return " ".join(x_list)

crawl_df['meta_desc'] =  crawl_df['meta_desc'].apply(lambda x: make_to_base(x))

x =' '.join(crawl_df.meta_desc)

xy = str(x)
z =  xy.split()

z

"""count the distiboution of words"""

count = crawl_df.groupby(['meta_desc']).count()

df200 = pd.DataFrame(columns = ['word', 'frequency'])

import numpy as np

df200=pd.Series(np.concatenate([x.split() for x in crawl_df.meta_desc])).value_counts()[10:20]

df200.index

import plotly.express as px

"""## data visulization"""

fig = px.bar(df200, x=df200.index,y=df200,title="Top 10 frequent  Words in Meta decription frequent ", labels={"y": "Count", "x": "Top 10 Words Stems"},
              )
fig.show()

